{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Monte Carlo Algorithms in Connect4\n",
        "\n",
        "In this notebook, we will explore Monte Carlo Tree Search in the classic Connect4 game! If you are unfamiliar with the game, you can find the [rules](https://www.hasbro.com/common/instruct/ConnectFour.PDF) here and [watch](https://www.youtube.com/watch?v=ylZBRUJi3UQ) a live playing of the game here.\n",
        "\n",
        "![picture](https://miro.medium.com/v2/resize:fit:2000/1*ZhiICBnWZLN4Mz93xozWkA.png)\n",
        "\n",
        "## Game Model\n",
        "\n",
        "First, we will construct our game state being the Connect4 board! Some key features we want to record in our board are:\n",
        "\n",
        "* Current turn (P1 or P2)\n",
        "* Terminal state\n",
        "* All the positions of the pieces\n",
        "  * Differentiating P1 and P2 piece\n",
        "* Rewards of state!\n",
        "  * (+1) for Player 1 win\n",
        "  * (-1) for Player 2 win\n",
        "  * (0) for tie! or non-terminal\n",
        "\n",
        "One implementation feature to note is that we keep track of each columns height. This makes it easier for us to resolve where a new piece will go into a column as well as whether the column is full. The time complexity goes from O(Board_Height) to O(1).\n",
        "\n",
        "Another item to note is that win conditions can only change around the most recently played piece. That is, a Connect4 state will only go terminal if the most recently played piece directly caused it to go terminal by a win/tie condition. Therefore, we are able to exploit this fact and whenever we want to check whether a player has won, we only need to look around the most recently played piece in all 4 directions (up-down, left-right, both diagonals). The time complexity goes from O(Board_Height * Board_Width) to O(1).\n",
        "\n",
        "These optimizations help the game run faster and thus allow more time for the Monte Carlo Tree Search to learn later on! This is especially important if you decide to make the board much bigger than a simple 7x6."
      ],
      "metadata": {
        "id": "xns4Y-kkD11R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aorbdEoXDYWc"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "class Board:\n",
        "    \"\"\"\n",
        "    Connect 4 Game Board\n",
        "    \"\"\"\n",
        "\n",
        "    P1symbol = \"X\"\n",
        "    P2symbol = \"O\"\n",
        "    WinLength = 4\n",
        "    TerminalTurn = 2\n",
        "\n",
        "    def __init__(self, width, height) -> None:\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.board = [[\"+\" for _ in range(width)] for _ in range(height)]\n",
        "        self.column_heights = [0 for _ in range(width)]\n",
        "\n",
        "        # 0 = P1, 1 = P2, 2 = Terminal\n",
        "        self.turn = 0\n",
        "        self.reward = 0\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"\n",
        "        Print the board\n",
        "        \"\"\"\n",
        "        board = [\" \".join([str(x) for x in row]) for row in reversed(self.board)]\n",
        "        board.append(\"-\" * (self.width*2-1))\n",
        "        board.append(\" \".join([str(x) for x in range(self.width)]))\n",
        "        board.append(\"-\" * (self.width*2-1))\n",
        "        return \"\\n\".join(board)\n",
        "\n",
        "    def play(self, col) -> bool:\n",
        "        \"\"\"\n",
        "        Play a piece in the column\n",
        "        \"\"\"\n",
        "        height = self.column_heights[col]\n",
        "        piece = Board.P1symbol if self.turn == 0 else Board.P2symbol\n",
        "\n",
        "        # Out of bounds\n",
        "        if col < 0 or col >= self.width:\n",
        "            return False\n",
        "\n",
        "        # Column is full\n",
        "        if height >= self.height:\n",
        "            return False\n",
        "\n",
        "        # Game already over!\n",
        "        if self.is_terminal():\n",
        "            return False\n",
        "\n",
        "        winner = self.check_win(col)\n",
        "\n",
        "        # Game over! player won --> terminal\n",
        "        if winner:\n",
        "            points = 1 if self.turn == 0 else -1\n",
        "            self.set_terminal(points)\n",
        "\n",
        "        # Play turn!\n",
        "        self.board[height][col] = piece\n",
        "        self.column_heights[col] += 1\n",
        "        self.update_turn()\n",
        "\n",
        "        # Game over! board is full --> terminal\n",
        "        if len(self.get_actions()) == 0:\n",
        "            self.set_terminal(0)\n",
        "\n",
        "        return True\n",
        "\n",
        "    def set_terminal(self, reward) -> None:\n",
        "        \"\"\"\n",
        "        Set the board to terminal\n",
        "        \"\"\"\n",
        "        self.turn = Board.TerminalTurn\n",
        "        self.reward = reward\n",
        "\n",
        "\n",
        "    def get_turn(self) -> str:\n",
        "        \"\"\"\n",
        "        Return the current turn\n",
        "        \"\"\"\n",
        "        turn_map = {\n",
        "            0 : Board.P1symbol,\n",
        "            1 : Board.P2symbol,\n",
        "            Board.TerminalTurn : \"Terminal\"\n",
        "        }\n",
        "        return turn_map[self.turn]\n",
        "\n",
        "    def actor(self) -> int:\n",
        "        \"\"\"\n",
        "        Return the current actor\n",
        "        \"\"\"\n",
        "        return self.turn\n",
        "\n",
        "    def get_actions(self) -> list:\n",
        "        \"\"\"\n",
        "        Return a list of valid moves\n",
        "        \"\"\"\n",
        "        return [col for col in range(self.width) if self.column_heights[col] < self.height]\n",
        "\n",
        "    def successor(self, col):\n",
        "        \"\"\"\n",
        "        Return a new board with the move played\n",
        "        \"\"\"\n",
        "        new_board = copy.deepcopy(self)\n",
        "        new_board.play(col)\n",
        "        return new_board\n",
        "\n",
        "    def is_terminal(self) -> bool:\n",
        "        \"\"\"\n",
        "        Check if the game is over\n",
        "        \"\"\"\n",
        "        return self.turn == 2 or len(self.get_actions()) == 0\n",
        "\n",
        "    def update_turn(self) -> None:\n",
        "        \"\"\"\n",
        "        Update the turn\n",
        "        \"\"\"\n",
        "        if self.turn != Board.TerminalTurn:\n",
        "            self.turn = (self.turn + 1) % 2\n",
        "\n",
        "    def payoff(self) -> int:\n",
        "        \"\"\"\n",
        "        Return the reward\n",
        "        \"\"\"\n",
        "        return self.reward\n",
        "\n",
        "    def check_win(self, col) -> bool:\n",
        "        \"\"\"\n",
        "        Check if adding a piece to the column will win the game\n",
        "        \"\"\"\n",
        "\n",
        "        direction_groups = [\n",
        "            [(1,0), (-1,0)],    # Horizontal -\n",
        "            [(0,1), (0,-1)],    # Vertical   |\n",
        "            [(1,1), (-1,-1)],   # Diagonal   /\n",
        "            [(1,-1), (-1,1)]    # Diagonal   \\\n",
        "        ]\n",
        "\n",
        "        piece = Board.P1symbol if self.turn == 0 else Board.P2symbol\n",
        "        row = self.column_heights[col]\n",
        "\n",
        "        # Cannot win if the column is full\n",
        "        if row >= self.height:\n",
        "            return False\n",
        "\n",
        "        for direction_group in direction_groups:\n",
        "\n",
        "            # Count of number of continuous pieces in a line\n",
        "\n",
        "            count = 1\n",
        "            for dx, dy in direction_group:\n",
        "                for i in range(1, 4):\n",
        "                    x = col + i * dx\n",
        "                    y = row + i * dy\n",
        "                    if x < 0 or x >= self.width or y < 0 or y >= self.height:\n",
        "                        break\n",
        "                    if self.board[y][x] == piece:\n",
        "                        count += 1\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "            if count >= Board.WinLength:\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def check_opposing_win(self, col) -> bool:\n",
        "        \"\"\"\n",
        "        Check if adding a piece to the column will cause the opponent to win\n",
        "        \"\"\"\n",
        "        self.update_turn()\n",
        "        result = self.check_win(col)\n",
        "        self.update_turn()\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agents\n",
        "\n",
        "Next, we will define the standard agents for this game which will include:\n",
        "\n",
        "1. **Human**: Allows people to interact in the CLI to play against any of the agents!\n",
        "\n",
        "*Note: humans can not play on a default collab account, you must download the python files and run in your own terminal to play!*\n",
        "\n",
        "2. **Random**: As you may guess, the random agent randomly places pieces into columns until the game is over. They pay no regard to the relative position of pieces.\n",
        "\n",
        "3. **RandomGreedy**: Similar to the last agent, this one will play mostly randomly until there is a critical condition. There are 3 key conditions checked before each placement in the following priority: (1) Will I win if I place a piece in this column. (2) Will the opponent win if I don't place a piece in this column. (3) Will the opponent win if I do place a piece in this column.\n",
        "\n",
        "4. **MonteCarlo**: This agent will use Monte Carlo methods of simulated numerous playouts of the game to figure out what move results in the best rewards for the agent!"
      ],
      "metadata": {
        "id": "28DFpOJtEabn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random as rand\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class InvalidMoveException(Exception):\n",
        "\n",
        "    def __init__(self, move):\n",
        "        self.move = move\n",
        "\n",
        "class Agent(ABC):\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_move(self, board: Board) -> int:\n",
        "        pass\n",
        "\n",
        "class HumanAgent(Agent):\n",
        "\n",
        "    def get_move(self, board: Board) -> int:\n",
        "\n",
        "        valid_moves = board.get_actions()\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                move = int(input(\"Enter a move: \"))\n",
        "                if move in valid_moves:\n",
        "                    return move\n",
        "                else:\n",
        "                    raise InvalidMoveException(move)\n",
        "            except InvalidMoveException:\n",
        "                print(\"Invalid move! Must be within\", valid_moves)\n",
        "            except ValueError:\n",
        "                print(\"Invalid move! Must be an integer.\")\n",
        "\n",
        "class RandomAgent(Agent):\n",
        "\n",
        "    def get_move(self, board: Board) -> int:\n",
        "        moves = board.get_actions()\n",
        "        return rand.choice(moves)\n",
        "\n",
        "class RandomGreedyAgent(Agent):\n",
        "\n",
        "    def get_move(self, board: Board) -> int:\n",
        "\n",
        "        moves = board.get_actions()\n",
        "\n",
        "        # Prioritize important moves\n",
        "        for move in moves:\n",
        "            # Win the game!\n",
        "            if board.check_win(move):\n",
        "                return move\n",
        "            # Block the opponent from winning!\n",
        "            if board.check_opposing_win(move):\n",
        "                return move\n",
        "\n",
        "        # Remove moves that lead to a loss the next turn!\n",
        "        good_moves = []\n",
        "        for move in moves:\n",
        "            new_board = board.successor(move)\n",
        "            opponent_win = new_board.check_win(move)\n",
        "            if not opponent_win:\n",
        "                good_moves.append(move)\n",
        "\n",
        "        if good_moves:\n",
        "            return rand.choice(good_moves)\n",
        "        else:\n",
        "            return rand.choice(moves)\n"
      ],
      "metadata": {
        "id": "-T8JCsluEaJa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monte Carlo Tree Search\n",
        "\n",
        "Monte Carlo Tree Search (MCTS) is a decision-making algorithm widely used in artificial intelligence, especially for games like Go and Chess. It strikes a balance between exploring new actions and exploiting known strategies. Here's how it works:\n",
        "\n",
        "**Selection**: Starting from the root node (the current state), the algorithm selects the most promising child node based on a specific criterion, such as the Upper Confidence Bound applied to Trees (UCT). This balances the exploration of less visited nodes and the exploitation of nodes with a high win rate. The formula is as follows:\n",
        "\n",
        "![picture](https://i.stack.imgur.com/lFPTK.png)\n",
        "\n",
        "**Expansion**: Upon reaching a leaf node (a state where the outcome is not yet known), the algorithm expands the tree by adding one or more child nodes, representing possible future states.\n",
        "\n",
        "**Simulation**: From the newly added nodes, the algorithm simulates random play-outs or games to the end, using a default or random policy to make decisions. This step estimates the value of the new node.\n",
        "\n",
        "**Backpropagation**: Finally, the results of these simulations are propagated back up the tree. Each node is updated with the new information, such as the win/loss ratio, helping to refine the decision-making process for future iterations.\n",
        "\n",
        "MCTS iteratively runs through these steps, building a tree of possibilities, until a termination condition is met such as a time limit or iteration count. This method allows AI to handle games with vast numbers of possible moves by focusing on the most promising strategies.\n",
        "\n",
        "![picture](https://www.researchgate.net/publication/320742905/figure/fig1/AS:631642972504115@1527606828915/Diagram-representing-the-4-steps-of-MCTS-In-the-first-two-steps-the-tree-is-traversed.png)"
      ],
      "metadata": {
        "id": "dqpf__l6Gedx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "# Note: Connect4 is set up as a zero-sum game where:\n",
        "# P1 is trying to maximize the score (+1)\n",
        "# P2 is trying to minimize the score (-1)\n",
        "\n",
        "class MonteCarloNode:\n",
        "\n",
        "    def __init__(self, state : Board, parent=None, parent_action=None):\n",
        "\n",
        "        self.state = state\n",
        "        self.parent = parent\n",
        "        self.parent_action = parent_action\n",
        "\n",
        "        self.total_visits = 0\n",
        "        self.total_rewards = 0\n",
        "\n",
        "        self.children = []\n",
        "\n",
        "        if self.state.is_terminal():\n",
        "            self.missing_child_actions = []\n",
        "        else:\n",
        "            self.missing_child_actions = self.state.get_actions()\n",
        "\n",
        "    def is_fully_expanded(self):\n",
        "        \"\"\"\n",
        "        Determines if the node is fully expanded.\n",
        "        \"\"\"\n",
        "        return len(self.missing_child_actions) == 0\n",
        "\n",
        "    def get_average_reward(self):\n",
        "        \"\"\"\n",
        "        Returns the average reward of a node.\n",
        "        \"\"\"\n",
        "        if self.total_visits == 0:\n",
        "            return 0\n",
        "        return self.total_rewards / self.total_visits\n",
        "\n",
        "    def get_best_average_child(self):\n",
        "        \"\"\"\n",
        "        Returns the child node with the best average reward.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.state.actor() == 0:\n",
        "            node = max(self.children, key = lambda x: x.get_average_reward())\n",
        "        else:\n",
        "            node = min(self.children, key = lambda x: x.get_average_reward())\n",
        "        return node\n",
        "\n",
        "    def get_ucb_value(self, parent_total_visits, parent_actor):\n",
        "        \"\"\"\n",
        "        Returns the UCB value of the node using parents visits and actor\n",
        "        \"\"\"\n",
        "\n",
        "        average_reward = self.get_average_reward()\n",
        "        exploration_term = math.sqrt(2 * math.log(parent_total_visits) / self.total_visits)\n",
        "        if parent_actor == 0:\n",
        "            value = average_reward + exploration_term\n",
        "        else:\n",
        "            # Note: Player 2 priotizes negative rewards\n",
        "            # Therefore we must subtract the exploration term to encourage exploring!\n",
        "            value = average_reward - exploration_term\n",
        "        return value\n",
        "\n",
        "    def get_best_ucb_child(self):\n",
        "        \"\"\"\n",
        "        Returns the child node with the best UCB value.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.state.actor() == 0:\n",
        "            node = max(self.children, key = lambda x: x.get_ucb_value(self.total_visits, self.state.actor()))\n",
        "        else:\n",
        "            node = min(self.children, key = lambda x: x.get_ucb_value(self.total_visits, self.state.actor()))\n",
        "        return node\n",
        "\n",
        "    def expand(self):\n",
        "        \"\"\"\n",
        "        Expands the node by adding a new child node from an unexplored action.\n",
        "        \"\"\"\n",
        "        action = self.missing_child_actions.pop()\n",
        "        next_state = self.state.successor(action)\n",
        "        child_node = MonteCarloNode(next_state, parent=self, parent_action=action)\n",
        "        self.children.append(child_node)\n",
        "        return child_node\n",
        "\n",
        "    def find_leaf_node(self):\n",
        "        \"\"\"\n",
        "        Returns the leaf node of the tree using UCB values. Expands node if necessary!\n",
        "        \"\"\"\n",
        "\n",
        "        current_node = self\n",
        "        while not current_node.state.is_terminal():\n",
        "\n",
        "            # If node is within the tree! with missing children\n",
        "            if not current_node.is_fully_expanded():\n",
        "                return current_node.expand()\n",
        "            else:\n",
        "                # Traverse down the tree!\n",
        "                current_node = current_node.get_best_ucb_child()\n",
        "\n",
        "        # The MCTS leaf node belongs to a terminal state!\n",
        "        return current_node\n",
        "\n",
        "    def simulate(self):\n",
        "        \"\"\"\n",
        "        Returns the terminal value of the node by randomly simulating game.\n",
        "        \"\"\"\n",
        "\n",
        "        state = self.state\n",
        "        while not state.is_terminal():\n",
        "            state = state.successor(rand.choice(state.get_actions()))\n",
        "        return state.payoff()\n",
        "\n",
        "    def update_rewards(self, reward):\n",
        "        \"\"\"\n",
        "        Updates the total reward and total visits of the node and all its parents.\n",
        "        \"\"\"\n",
        "\n",
        "        parent_node = self\n",
        "        while parent_node is not None:\n",
        "            parent_node.total_rewards += reward\n",
        "            parent_node.total_visits += 1\n",
        "            parent_node = parent_node.parent\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "\n",
        "        ucb_value = \"Undefined\"\n",
        "        if self.parent:\n",
        "            ucb_value = self.get_ucb_value(self.parent.total_visits, self.parent.state.actor())\n",
        "\n",
        "        string_form = f\"\"\"\n",
        "        Node: {self.state}\n",
        "        Total Visits: {self.total_visits}\n",
        "        Average Reward: {self.get_average_reward()}\n",
        "        UCB Value: {ucb_value}\n",
        "        Parent Action: {self.parent_action}\n",
        "        Actor: {self.state.actor()}\n",
        "        Children: {len(self.children)}\n",
        "        \"\"\"\n",
        "\n",
        "        return string_form\n",
        "\n",
        "\n",
        "def mcts_policy(time_duration):\n",
        "\n",
        "    def fxn(initial_position: Board):\n",
        "\n",
        "        start_time = time.time()\n",
        "        root = MonteCarloNode(initial_position, None)\n",
        "\n",
        "        while time.time() - start_time < time_duration:\n",
        "\n",
        "            # Gets the leaf node in the tree (step 1 & 2)\n",
        "            node = root.find_leaf_node()\n",
        "\n",
        "            # Determines the random terminal value of the node (step 3)\n",
        "            reward = node.simulate()\n",
        "\n",
        "            # Updates the rewards of the parents (step 4)\n",
        "            node.update_rewards(reward)\n",
        "\n",
        "        node = root.get_best_average_child()\n",
        "\n",
        "        return node.parent_action\n",
        "\n",
        "    return fxn\n"
      ],
      "metadata": {
        "id": "RQJKOU3kGk6X"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the Monte Carlo Tree Search algorithm built, we can configure the agent to use the function! If you are looking for more resources to learn MCTS, you can watch the following [video](https://www.youtube.com/watch?v=onBYsen2_eA) on YouTube!"
      ],
      "metadata": {
        "id": "m1icO6ZWGvMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MonteCarloAgent(Agent):\n",
        "\n",
        "    def __init__(self, duration, random_move_prob=0.05) -> None:\n",
        "        super().__init__()\n",
        "        self.random_move_prob = random_move_prob\n",
        "        self.policy_fxn = mcts_policy(duration)\n",
        "\n",
        "    def get_move(self, board: Board) -> int:\n",
        "\n",
        "        # Occasional random move to introduce non-determinism\n",
        "        if rand.random() < self.random_move_prob:\n",
        "            return rand.choice(board.get_actions())\n",
        "        else:\n",
        "            return self.policy_fxn(board)"
      ],
      "metadata": {
        "id": "K9TFFayOGtHi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Game Play!"
      ],
      "metadata": {
        "id": "Jc5fba4JG6zV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "class Game:\n",
        "\n",
        "    def __init__(self, p1_agent: Agent, p2_agent: Agent, width: int, height: int, turn_sleep: bool = False) -> None:\n",
        "\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.p1_agent = p1_agent\n",
        "        self.p2_agent = p2_agent\n",
        "        self.turn_sleep = turn_sleep\n",
        "\n",
        "    def play(self, extra_print=False) -> int:\n",
        "        \"\"\"\n",
        "        Plays the game until a terminal state is reached:\n",
        "            Returns 1 if P1 wins, -1 if P2 wins, 0 if tie\n",
        "        \"\"\"\n",
        "\n",
        "        board = Board(self.width, self.height)\n",
        "\n",
        "        while True:\n",
        "\n",
        "            if self.turn_sleep:\n",
        "                time.sleep(1)\n",
        "\n",
        "            if extra_print:\n",
        "                print(board)\n",
        "\n",
        "            # Terminal Tie\n",
        "            if len(board.get_actions()) == 0:\n",
        "                if extra_print:\n",
        "                    print(\"Tie!\")\n",
        "                return 0\n",
        "\n",
        "            # Get move from agent\n",
        "            current_turn = board.get_turn()\n",
        "            agent = self.p1_agent if current_turn == Board.P1symbol else self.p2_agent\n",
        "            move = agent.get_move(board)\n",
        "\n",
        "            # Check if winning move & play!\n",
        "            winner = board.check_win(move)\n",
        "            valid = board.play(move)\n",
        "\n",
        "            if winner:\n",
        "                if extra_print:\n",
        "                    print(board)\n",
        "                    print(\"-\"*10)\n",
        "                    print(current_turn, \"wins!\")\n",
        "                    print(\"-\"*10)\n",
        "                return 1 if current_turn == Board.P1symbol else -1\n",
        "\n",
        "            if not valid:\n",
        "                print(\"Warning: invalid move:\", move)\n",
        "\n",
        "def simulate_matchups(p1: Agent, p2: Agent, width: int, height: int, count: int) -> tuple:\n",
        "    game = Game(p1, p2, width, height)\n",
        "    p1_wins, p2_wins, ties = 0, 0, 0\n",
        "    for _ in range(count):\n",
        "        result = game.play()\n",
        "        if result == 1:\n",
        "            p1_wins += 1\n",
        "        elif result == -1:\n",
        "            p2_wins += 1\n",
        "        else:\n",
        "            ties += 1\n",
        "    return p1_wins, p2_wins, ties"
      ],
      "metadata": {
        "id": "zNddFc13G8xp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we will simulate matchups between all of the agents!"
      ],
      "metadata": {
        "id": "MQxGKCTuIDHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "width = 7\n",
        "height = 6\n",
        "count = 100\n",
        "monte_carlo_time = 0.15\n",
        "randomness = 0  # Option to handicap the monte carlo agent to randomly play sub-optimal!\n",
        "\n",
        "human = HumanAgent()\n",
        "random = RandomAgent()\n",
        "greedy = RandomGreedyAgent()\n",
        "monte = MonteCarloAgent(monte_carlo_time, randomness)\n",
        "\n",
        "setups = [\n",
        "    ((random, random), (\"Random\", \"Random\")),\n",
        "    ((random, greedy), (\"Random\", \"Greedy\")),\n",
        "    ((greedy, greedy), (\"Greedy\", \"Greedy\")),\n",
        "    ((random, monte), (\"Random\", \"Monte Carlo\")),\n",
        "    ((greedy, monte), (\"Greedy\", \"Monte Carlo\")),\n",
        "    ((monte, monte), (\"Monte Carlo\", \"Monte Carlo\")),\n",
        "]\n",
        "\n",
        "for config, names in setups:\n",
        "    p1_wins, p2_wins, ties = simulate_matchups(*config, width, height, count)\n",
        "    print(f\"{names[0]}: {round(p1_wins/count*100, 3)}%\",\n",
        "        f\"{names[1]}: {round(p2_wins/count*100, 3)}%\",\n",
        "        f\"Ties: {round(ties/count*100, 3)}%\",\n",
        "        sep=\"\\t\"\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9X3Ha81HoUI",
        "outputId": "6df70580-93e0-40f3-8f33-48c8c3ad0f9a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random: 51.0%\tRandom: 48.0%\tTies: 1.0%\n",
            "Random: 2.0%\tGreedy: 98.0%\tTies: 0.0%\n",
            "Greedy: 41.0%\tGreedy: 49.0%\tTies: 10.0%\n",
            "Random: 0.0%\tMonte Carlo: 100.0%\tTies: 0.0%\n",
            "Greedy: 2.0%\tMonte Carlo: 97.0%\tTies: 1.0%\n",
            "Monte Carlo: 56.0%\tMonte Carlo: 39.0%\tTies: 5.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you don't have time to run the above code (can take up to 10 mins), the expected output looks something like the following:\n",
        "\n",
        "\\begin{array}{|c|c|} \\hline\n",
        "Random: 57.0\\% & Random: 43.0\\% & Ties: 0.0\\% \\\\\n",
        "Random: 6.0\\% & Greedy: 94.0\\% & Ties: 0.0\\% \\\\\n",
        "Greedy: 45.0\\% & Greedy: 39.0\\% & Ties: 16.0\\% \\\\\n",
        "Random: 0.0\\% & Monte Carlo: 100.0\\% & Ties: 0.0\\% \\\\\n",
        "Greedy: 6.0\\% & Monte Carlo: 93.0\\% & Ties: 1.0\\% \\\\\n",
        "Monte Carlo: 54.0\\% & Monte Carlo: 39.0\\% & Ties: 7.0\\% \\\\ \\hline\n",
        "\\end{array}\n",
        "\n",
        "One interesting to note is that Player 1 tends to have an advantage over Player 2 when the same algoritms play against each other! This is due to the fact that the game is actually solved and under optimal play, Player 1 has a 100% chance of winning! Here is a [reference](https://connect4.gamesolver.org/en/) to an AI which can solve the game from any position!"
      ],
      "metadata": {
        "id": "LT4_CV3GKi01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Greedy vs Greedy with extra iterations\n",
        "# Goal: Test advantage for P1 in a larger sample size!\n",
        "p1_wins, p2_wins, ties = simulate_matchups(greedy, greedy, width, height, count*100)\n",
        "print(f\"Greedy: {round(p1_wins/(count*100), 3)}%\",\n",
        "    f\"Greedy: {round(p2_wins/(count*100), 3)}%\",\n",
        "    f\"Ties: {round(ties/(count*100), 3)}%\",\n",
        "    sep=\"\\t\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWu-haedkA2M",
        "outputId": "1dc7c48d-0ff1-4fc1-df82-dbae02c53b1d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy: 0.464%\tGreedy: 0.414%\tTies: 0.122%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code chunk runs the quicker greedy algorithms for 100 times more iterations than previously to get a better sense of how skewed the game is in favor of the first mover."
      ],
      "metadata": {
        "id": "JaH3DRw1kQZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Greedy vs MonteCarlo Agent with 66% less simulation time\n",
        "# Goal: Test worse performance of MCTS under lower learning!\n",
        "\n",
        "monte_carlo_time = monte_carlo_time/3\n",
        "p1_wins, p2_wins, ties = simulate_matchups(greedy, MonteCarloAgent(monte_carlo_time, randomness), width, height, count)\n",
        "print(f\"Greedy: {round(p1_wins/count*100, 3)}%\",\n",
        "    f\"MonteCarlo: {round(p2_wins/count*100, 3)}%\",\n",
        "    f\"Ties: {round(ties/count*100, 3)}%\",\n",
        "    sep=\"\\t\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jd4HHzWwKhjt",
        "outputId": "5b4cd546-977f-432b-fcad-c103d26f81ca"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy: 12.0%\tMonteCarlo: 82.0%\tTies: 6.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another important parameter with Monte Carlo Tree Search is the amount of time allowed for simulations. We may expect that as we give the model more time, it performs better. Similarly, if we reduce time, it will perform worse. The following is an example output from the above code chunk:\n",
        "\n",
        "\\begin{array}{|c|c|} \\hline\n",
        "Greedy: 29.0\\% & MonteCarlo: 65.0\\%\t& Ties: 6.0\\% \\\\ \\hline\n",
        "\\end{array}\n",
        "\n",
        "We observe that when we gave the algorithm 66% less time and the Greedy agent started winning almost 5 times more often! (6% to about 30%). This is consistent with our understanding of MCTS. If you have time to burn, you could give the MCTS Agent much longer time to train and see how well the model performs then!"
      ],
      "metadata": {
        "id": "35rvnerJP1Td"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "By following through this tutorial you will have learned how Monte Carlo Tree Search works by traversing through a tree using exploitation & exploration, then expanding the tree, simulating a playout of the game, and updating the rewards up the path. A big advantage of Monte Carlo Tree Search is that we the flexibility of changing the amount of time it learns depending on the desired use case! There is a tradeoff between time and performance as seen by the examples above. This tutorial also gives guidance on how to design a  state for any type of AI algorithm.\n",
        "\n",
        "In recent years, Monte Carlo tree searcb has been combined with neural networks and used in multiple board games like Backgammon, Chess, Checkers, Contract Bridge, Go, and Scrabble to efficiently find optimal play!"
      ],
      "metadata": {
        "id": "mlgmu0CYI5Kw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Future Extensions\n",
        "\n",
        "Some interesting extensions to this baseline tutorial include:\n",
        "\n",
        "1. **Expectimax or Neural Net**: I believe that it is possible for a different algorithm to truly find the optimal move for Connect4 in a reasonable amount of time. It has been concluded that the game is solved with a first mover advantage. That is, under optimal play the first player will always win! The MCTS algorithm was still close to a 50/50 split when playing against itself even though Player 1 should in theory win 100% of the time. Therefore some other algorithms like Expectimax to find the value of each state or possibly training a neural network would be better!\n",
        "\n",
        "2. **Greedy Algorithm Improvements**: Currently, the greedy algorithm only considers whether its move is going to result in a terminal state in the current or next turn and takes action to maximize its own rewards. Otherwise, it randomly places pieces. One improvement that could be made to the greedy algorithm could involve prioritizing putting pieces where they are connecting to other of your own pieces since that is the win condition of the game after all.\n",
        "\n",
        "3. **Connect4 Variants**: All of the examples in the tutorial are based on the standard Connect4 game size of 7 width and 6 height. It could be interesting to change some of the rules of the game and see how that influences the effectiveness of MCTS, changes the rate of ties, and changes the bias towards P1 winning. Factors that could be changed include the board size (both width and height), the win condition (Connect4 â†’ Connect5), the turn mechanism (each player has 2 sequential turns instead), etc.\n",
        "\n",
        "4. **MCTS Improvements**: Improve efficiency of MCTS by only allowing 1-1 mapping of state to MCTS node. Throughout all the possible different playouts of Connect4, there are numerous ways in which we can end up in a particular game state. Currently, these nodes are all separated throughout the tree even though they represent the same underlying game state. One way to improve the efficiency of the tree search, we could combine all these nodes so that they are better able to use the simulation data!\n"
      ],
      "metadata": {
        "id": "BCOQGS0Ck6va"
      }
    }
  ]
}